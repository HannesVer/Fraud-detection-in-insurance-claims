{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #print everything\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #ignore warnings\n",
    "\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\Hannes Verschueren\\Documents\\SPRING SEMESTER 2021\\Advanced Analytics\\Final Assignment 2') #set working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', 500) #show all columns\n",
    "\n",
    "# import preprocessed data\n",
    "\n",
    "X_train_pp=pd.read_csv(\"X_train_pp.csv\")\n",
    "X_eval_pp=pd.read_csv(\"X_eval_pp.csv\")\n",
    "X_test_pp=pd.read_csv(\"X_test_pp.csv\")\n",
    "test_id=pd.read_csv(\"test_id.csv\")\n",
    "eval_id=pd.read_csv(\"eval_id.csv\")\n",
    "y_test=pd.read_csv(\"y_test.csv\")\n",
    "y_train=pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv',sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = train.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "\n",
    "plt.savefig('correlation_plot_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#claim_language = float64??\n",
    "\n",
    "X_train_pp.claim_language=X_train_pp.claim_language.astype(\"object\")\n",
    "X_test_pp.claim_language=X_test_pp.claim_language.astype(\"object\")\n",
    "X_eval_pp.claim_language=X_eval_pp.claim_language.astype(\"object\")\n",
    "\n",
    "#claim_date_occured_day_of_the_week and claim_date_registered_day_of_the_week are int64, have to become categorical\n",
    "\n",
    "X_train_pp.claim_date_occured_day_of_the_week=X_train_pp.claim_date_occured_day_of_the_week.astype(\"object\")\n",
    "X_test_pp.claim_date_occured_day_of_the_week=X_test_pp.claim_date_occured_day_of_the_week.astype(\"object\")\n",
    "X_eval_pp.claim_date_occured_day_of_the_week=X_eval_pp.claim_date_occured_day_of_the_week.astype(\"object\")\n",
    "\n",
    "X_train_pp.claim_date_registered_day_of_the_week=X_train_pp.claim_date_registered_day_of_the_week.astype(\"object\")\n",
    "X_test_pp.claim_date_registered_day_of_the_week=X_test_pp.claim_date_registered_day_of_the_week.astype(\"object\")\n",
    "X_eval_pp.claim_date_registered_day_of_the_week=X_eval_pp.claim_date_registered_day_of_the_week.astype(\"object\")\n",
    "\n",
    "X_train_pp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_with_missing_values_categorical(df):\n",
    "    \n",
    "    numeric_columns = df.select_dtypes(include='number').columns\n",
    "    categorical_columns=df.select_dtypes(include='object').columns\n",
    "    df_numeric=df[numeric_columns]\n",
    "    df_categorical=df[categorical_columns]\n",
    "    df_rest=df.drop(numeric_columns,axis=1).drop(categorical_columns,axis=1)\n",
    "    \n",
    "    df_categorical=df_categorical.astype(\"string\").fillna(value=\"Unknown\").astype(\"category\")\n",
    "    \n",
    "    df=pd.concat([df_categorical,df_numeric,df_rest],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "X_train_pp=deal_with_missing_values_categorical(X_train_pp)\n",
    "X_test_pp=deal_with_missing_values_categorical(X_test_pp)\n",
    "X_eval_pp=deal_with_missing_values_categorical(X_eval_pp)\n",
    "\n",
    "X_train_pp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#initial weights\n",
    "y_train[\"weights\"]=y_train[\"claim_amount\"]*y_train[\"fraud\"]\n",
    "\n",
    "\n",
    "#adjusted weights\n",
    "mms=MinMaxScaler()\n",
    "y_train[\"scaled_claim_amount\"]=mms.fit_transform(y_train[[\"claim_amount\"]])\n",
    "y_train[\"weight_factor\"]=y_train[\"fraud\"].replace(0,0.1)\n",
    "y_train[\"adjusted_weights\"]=y_train[\"scaled_claim_amount\"]*y_train[\"weight_factor\"]\n",
    "\n",
    "y_train.sort_values(by=\"adjusted_weights\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearndf\n",
    "\n",
    "from sklearndf.transformation import (\n",
    "    ColumnTransformerDF,\n",
    "    OneHotEncoderDF,\n",
    "    SimpleImputerDF,\n",
    "    StandardScalerDF\n",
    ")\n",
    "from sklearndf.pipeline import PipelineDF\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = PipelineDF(steps=[\n",
    "    ('onehot', OneHotEncoderDF(sparse=False,handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = PipelineDF(steps=[\n",
    "    ('imputer', SimpleImputerDF(strategy='median')),\n",
    "    ('scaler', StandardScalerDF()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformerDF(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_pp_df=preprocessor.fit_transform(X_train_pp)\n",
    "ada=ADASYN()\n",
    "X_train_pp_df_sampled,y_train_sampled=ada.fit_sample(X_train_pp_df,y_train[\"fraud\"])\n",
    "\n",
    "X_train_pp_df_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapley values\n",
    "\n",
    "import shap\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "clf=XGBClassifier(colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=12, min_child_weight=1)\n",
    "clf.fit(X_train_pp_df_sampled,y_train_sampled)\n",
    "\n",
    "#https://github.com/slundberg/shap/issues/1215\n",
    "mybooster = clf.get_booster()\n",
    "model_bytearray = mybooster.save_raw()[4:]\n",
    "def myfun(self=None):\n",
    "    return model_bytearray\n",
    "mybooster.save_raw = myfun\n",
    "\n",
    "shap_values = shap.TreeExplainer(mybooster).shap_values(X_train_pp_df_sampled)\n",
    "shap.summary_plot(shap_values, X_train_pp_df_sampled, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with class imbalance - ADASYN vs SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.over_sampling import ADASYN,SMOTE\n",
    "\n",
    "# Instanciate a PCA object for the sake of easy visualisation\n",
    "pca = PCA(n_components=2)\n",
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "X_vis = pca.fit_transform(X_train_pp_df)\n",
    "\n",
    "# ADASYN\n",
    "ada = ADASYN()\n",
    "X_resampled, y_resampled = ada.fit_sample(X_train_pp_df, y_train[\"fraud\"])\n",
    "X_res_vis = pca.transform(X_resampled)\n",
    "\n",
    "# SMOTE\n",
    "smo=SMOTE(k_neighbors=10)\n",
    "X_resampled_smo, y_resampled_smo = smo.fit_sample(X_train_pp_df, y_train[\"fraud\"])\n",
    "X_res_vis_smo = pca.transform(X_resampled_smo)\n",
    "\n",
    "# Three subplots, unpack the axes array immediately\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "c0 = ax1.scatter(X_vis[y_train[\"fraud\"] == 0, 0], X_vis[y_train[\"fraud\"] == 0, 1], label=\"Class #0\",\n",
    "                 alpha=0.5)\n",
    "c1 = ax1.scatter(X_vis[y_train[\"fraud\"] == 1, 0], X_vis[y_train[\"fraud\"] == 1, 1], label=\"Class #1\",\n",
    "                 alpha=0.5)\n",
    "ax1.set_title('Original set')\n",
    "\n",
    "ax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n",
    "            label=\"Class #0\", alpha=.5)\n",
    "ax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n",
    "            label=\"Class #1\", alpha=.5)\n",
    "ax2.set_title('ADASYN')\n",
    "\n",
    "ax3.scatter(X_res_vis_smo[y_resampled_smo == 0, 0], X_res_vis_smo[y_resampled_smo == 0, 1],\n",
    "            label=\"Class #0\", alpha=.5)\n",
    "ax3.scatter(X_res_vis_smo[y_resampled_smo == 1, 0], X_res_vis_smo[y_resampled_smo == 1, 1],\n",
    "            label=\"Class #1\", alpha=.5)\n",
    "ax3.set_title('SMOTE')\n",
    "\n",
    "# make nice plotting\n",
    "for ax in (ax1, ax2,ax3):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    ax.set_xlim([-10, 10])\n",
    "    ax.set_ylim([-6, 8])\n",
    "\n",
    "plt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n",
    "              ncol=2, labelspacing=0.)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.savefig(\"oversampling_methods.jpg\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL FUNCTIONS\n",
    "\n",
    "# function to evaluate performance of model on own test set\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_outcome(y_test,y_pred,y_pred_proba,test_id):\n",
    "    \n",
    "    #AUC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[\"fraud\"], y_pred, pos_label=1)\n",
    "    AUC=metrics.auc(fpr, tpr)\n",
    "    \n",
    "    #Custom score:\n",
    "    index_list=list(y_test.index)\n",
    "    y_pred_df=pd.DataFrame(y_pred,columns=[\"y_pred\"])\n",
    "    y_pred_proba_df=pd.DataFrame(y_pred_proba[:,1],columns=[\"y_pred_proba\"])\n",
    "    index_df=pd.DataFrame(index_list,columns=[\"index_list\"])\n",
    "    y_pred_and_pred_proba_df=pd.concat([y_pred_proba_df,y_pred_df,index_df],axis=1).set_index(\"index_list\")\n",
    "    to_evaluate=pd.concat([test_id,y_test,y_pred_and_pred_proba_df],axis=1)\n",
    "    \n",
    "    y_pred_proba_df=pd.DataFrame(y_pred_proba[:,0],columns=[\"y_pred_proba\"])\n",
    "    to_evaluate=to_evaluate.sort_values(by=[\"y_pred_proba\"],ascending=False)\n",
    "    to_evaluate_top_100=to_evaluate.head(100)\n",
    "    \n",
    "    score=0\n",
    "    for i in range(len(to_evaluate_top_100.fraud)):\n",
    "        if to_evaluate_top_100.fraud.iloc[i] == to_evaluate_top_100.y_pred.iloc[i]:\n",
    "            score+= to_evaluate_top_100.claim_amount.iloc[i]\n",
    "    \n",
    "    print(\"AUC score of the model: \" + str(np.round(AUC,decimals=4))), print(\"Custom score:\"+ str(np.round(score,decimals=2)))\n",
    "    \n",
    "\n",
    "# leaderboard predictions to proper format\n",
    "\n",
    "def predictions_to_csv(index,y_pred_proba,model:str):\n",
    "    y_pred_proba_df=pd.DataFrame(y_pred_proba[:,1],columns=[\"PROB\"])\n",
    "    output=pd.concat([index.rename(columns={\"claim_id\":\"ID\"},inplace=False).astype(\"int64\"),y_pred_proba_df],axis=1)\n",
    "    output.to_csv(\"predictions-\"+model+\".csv\",sep=\",\",index=False)\n",
    "    return output.head(),print(\"Predictions saved to csv file :-)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL : Logistic regression (baseline)\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "     ('preprocessor', preprocessor),\n",
    "     ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"])#,clf__sample_weight=y_train[\"adjusted_weights\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- Logistic Regression (baseline)\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "#predictions_to_csv(eval_id,y_pred_proba_eval,\"logisticregression(baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate feature importance\n",
    "\n",
    "#to get the encodes features\n",
    "categorical_enc_features=pipe.named_steps[\"preprocessor\"].transformers_[1][1].named_steps['onehot'].get_feature_names(categorical_features).tolist()\n",
    "numerical_features=X_train_pp.select_dtypes(include='number').columns.tolist()\n",
    "features_enc=categorical_enc_features+numerical_features\n",
    "\n",
    "#to get the coefficients\n",
    "importance=pipe.named_steps[\"clf\"].coef_[0]\n",
    "\n",
    "#descriptive statistics of the coefficients\n",
    "from scipy import stats\n",
    "min_feature_importance=stats.describe(importance).minmax[0]\n",
    "max_feature_importance=stats.describe(importance).minmax[1]\n",
    "mean_feature_importance=stats.describe(importance).mean\n",
    "stddev_feature_importance=np.sqrt(stats.describe(importance).variance)\n",
    "\n",
    "#define boundaries for importance features\n",
    "upper_lower=max_feature_importance-2*stddev_feature_importance\n",
    "lower_upper=min_feature_importance+2*stddev_feature_importance\n",
    "\n",
    "#enumerate features considered to be important\n",
    "important_features=[]\n",
    "coefficients=[]\n",
    "for index,value in enumerate(importance):\n",
    "    if (value>upper_lower) or (value<lower_upper):\n",
    "        print(features_enc[index]+\": \"+str(value))\n",
    "        important_feature=[features_enc[index]]\n",
    "        important_features=important_features+important_feature\n",
    "        coefficient=[value]\n",
    "        coefficients=coefficients+coefficient\n",
    "\n",
    "df_feature_importance=pd.DataFrame(list(zip(important_features, coefficients)),columns=[\"important_features\",\"coefficients\"])\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig_dims = (40,20)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "importance_graph=sns.barplot(x=df_feature_importance.important_features,y=df_feature_importance.coefficients,ax=ax)\n",
    "importance_graph.set_xticklabels(labels=importance_graph.get_xticklabels(),rotation=90,fontsize=20)\n",
    "importance_graph.set_xlabel(\"Features\",fontsize=20)\n",
    "importance_graph.set_ylabel(\"Importance\",fontsize=20)\n",
    "importance_graph.set_title(\"Most important features\",fontsize=40,y=1.1)\n",
    "\n",
    "plt.savefig(\"feature_importance-logisticregression.jpeg\",bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL : Logistic regression + ADASYN\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "     ('preprocessor', preprocessor),\n",
    "     ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('sampling', ADASYN(n_neighbors=10,random_state=7)),\n",
    "        ('clf', LogisticRegression(penalty=\"l2\",solver=\"saga\"))\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'sampling__n_neighbors': [1, 2, 3, 5, 10],\n",
    "    'clf__penalty': [\"l1\", \"l2\", \"elasticnet\"],\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1,verbose=2,scoring=\"average_precision\")\n",
    "#search.fit(X_train_pp, y_train[\"fraud\"])\n",
    "#print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- Logistic Regression + ADASYN\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "predictions_to_csv(eval_id,y_pred_proba_eval,\"logisticregression+ADASYN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: XGBOOST + ADASYN (baseline)\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('sampling', ADASYN(random_state=7)),\n",
    "        ('clf', XGBClassifier())\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling__n_neighbors\": [1, 2, 3, 5, 10],\n",
    "    \"clf__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "    \"clf__max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"clf__min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"clf__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"clf__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "}\n",
    "#search = GridSearchCV(pipe, param_grid, n_jobs=-1,verbose=2,scoring=\"recall\",cv=3)\n",
    "#search.fit(X_train_pp, y_train[\"fraud\"])\n",
    "#print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- XGBOOST + ADASYN (baseline)\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "predictions_to_csv(eval_id,y_pred_proba_eval,\"XGBOOST+ADASYN(baseline)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: XGBOOST + ADASYN + CV (scoring=roc_auc)\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('sampling', ADASYN(n_neighbors=1,random_state=7)),\n",
    "        ('clf', XGBClassifier(colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=12, min_child_weight=1)) #optimal parameters GridSearchCV\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling__n_neighbors\": [1, 2, 3, 5, 10],\n",
    "    \"clf__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "    \"clf__max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"clf__min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"clf__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"clf__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "}\n",
    "#search = GridSearchCV(pipe, param_grid, n_jobs=-1,verbose=2,scoring=\"recall\",cv=3)\n",
    "#search.fit(X_train_pp, y_train[\"fraud\"])\n",
    "#print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- XGBOOST + ADASYN + CV(roc_auc)\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "predictions_to_csv(eval_id,y_pred_proba_eval,\"XGBoost+ADASYN+CV(roc_auc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get sample weights\n",
    "y_train[\"fraud\"].value_counts()\n",
    "num_fraud=205\n",
    "num_not_fraud=36770\n",
    "pos_weight=num_not_fraud/num_fraud\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: XGBOOST + sample weights (scoring=roc_auc)\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        #('sampling', ADASYN(n_neighbors=1,random_state=7)),\n",
    "        ('clf', XGBClassifier(colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=12, min_child_weight=1,scale_pos_weight=pos_weight)) #optimal parameters GridSearchCV\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling__n_neighbors\": [1, 2, 3, 5, 10],\n",
    "    \"clf__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "    \"clf__max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"clf__min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"clf__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"clf__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "}\n",
    "#search = GridSearchCV(pipe, param_grid, n_jobs=-1,verbose=2,scoring=\"recall\",cv=3)\n",
    "#search.fit(X_train_pp, y_train[\"fraud\"])\n",
    "#print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"],clf__sample_weight=y_train[\"adjusted_weights\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- XGBOOST + sample_weights\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "predictions_to_csv(eval_id,y_pred_proba_eval,\"XGBoost+sample_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomSearchCV on other computer with more cores\n",
    "print(\"Best parameters:  {'clf__min_child_weight': 7, 'clf__max_depth': 8, 'clf__learning_rate': 0.05, 'clf__gamma': 0.3, 'clf__colsample_bytree': 0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL: XGBOOST + PF\n",
    "\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.over_sampling import SMOTE, ADASYN #oversampling methods\n",
    "from sklearn.preprocessing import OneHotEncoder,PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "numeric_features = X_train_pp.select_dtypes(include='number').columns\n",
    "categorical_features = X_train_pp.select_dtypes(include='category').columns\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('poly',PolynomialFeatures(degree=2,interaction_only=True)),\n",
    "        ('sampling', ADASYN(n_neighbors=1,random_state=7)),\n",
    "        ('clf', XGBClassifier(colsample_bytree=0.5, gamma=0.3, learning_rate=0.05, max_depth=8, min_child_weight=7)) #optimal parameters GridSearchCV\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling__n_neighbors\": [1, 5, 10],\n",
    "    \"clf__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "    \"clf__max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "    \"clf__min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "    \"clf__gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "    \"clf__colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] \n",
    "}\n",
    "#search = GridSearchCV(pipe, param_grid, n_jobs=-1,verbose=2,scoring=‘roc_auc’,cv=3)\n",
    "#search.fit(X_train_pp, y_train[\"fraud\"])\n",
    "#print(\"Best parameters: \", search.best_params_)\n",
    "\n",
    "pipe.fit(X_train_pp,y_train[\"fraud\"])\n",
    "\n",
    "#own test set predictions\n",
    "y_pred=pipe.predict(X_test_pp)\n",
    "y_pred_proba=pipe.predict_proba(X_test_pp)\n",
    "\n",
    "#evaluation set predictions\n",
    "y_pred_proba_eval=pipe.predict_proba(X_eval_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate + save predictions to file -- XGBOOST + sample_weights\n",
    "evaluate_outcome(y_test,y_pred,y_pred_proba,test_id)\n",
    "predictions_to_csv(eval_id,y_pred_proba_eval,\"XGBoost+PF+RandomSearchCV(roc_auc)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pp.select_dtypes(include='number').columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
